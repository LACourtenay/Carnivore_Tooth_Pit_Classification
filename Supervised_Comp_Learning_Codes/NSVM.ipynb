{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Code by Lloyd A. Courtenay. Last Updated 10/02/2021\n",
    "### Python Code for the training of NSVMs\n",
    "\n",
    "Note for NSVM Tensorflow V.2.3. or higher is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions:\n",
      "Tensorflow 2.3.1\n",
      "Keras 2.4.0\n",
      "Numpy 1.17.3\n",
      "Pandas 0.25.2\n",
      "SciKit Learn 0.22.1\n",
      "HyperOpt: 0.2.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"\") # Set Directory\n",
    "\n",
    "# load libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.constraints import unit_norm\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers.experimental import RandomFourierFeatures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy.random import randint, rand, randn, random, choice\n",
    "from numpy import ones, zeros, vstack\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, cohen_kappa_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error\n",
    "from sklearn.svm import *\n",
    "import hyperopt\n",
    "from hyperopt import Trials, STATUS_OK, tpe, hp, fmin\n",
    "\n",
    "# Print package versions and ensure they have loaded correctly\n",
    "print(\"Versions:\")\n",
    "print(f\"Tensorflow {tf.__version__}\")\n",
    "print(f\"Keras {keras.__version__}\")\n",
    "print(f\"Numpy {np.__version__}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"SciKit Learn {sklearn.__version__}\")\n",
    "print(f\"HyperOpt: {hyperopt.__version__}\")\n",
    "\n",
    "# Set random number generator\n",
    "seed_value = 100\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyclic Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swish Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x, beta = 1):\n",
    "    return(x * K.sigmoid(beta * x))\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "get_custom_objects().update({\"swish\":Activation(swish)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "- Ensure that the file with PC Scores for training are in the directory and labelled \"Data_For_Train.csv\", using a commas for column delimiters.\n",
    "- Ensure that the file with PC Scores for testing are in the directory and labelled \"Data_For_Test.csv\", using a commas for column delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_value</th>\n",
       "      <th>sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hyena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Leopard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Licaon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Lion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key_value   sample\n",
       "0         0    Hyena\n",
       "1         1  Leopard\n",
       "2         2   Licaon\n",
       "3         3     Lion"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "dataframe = pd.read_csv(\"Data_for_Train.csv\", delimiter = \",\", header = None)\n",
    "dataframe = dataframe.drop(dataframe.index[0])\n",
    "X = dataframe.values[:,1:6].astype(float)\n",
    "Y = dataframe.iloc[:,0].astype(\"category\")\n",
    "test = pd.read_csv(\"Data_for_Test.csv\", delimiter = \",\", header = None)\n",
    "\n",
    "test = test.drop(test.index[0])\n",
    "X_test = test.values[:,1:6].astype(float)\n",
    "Y_test = test.iloc[:,0].astype(\"category\")\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y_encoded = encoder.transform(Y)\n",
    "dummy_Y =  tf.keras.utils.to_categorical(Y_encoded)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y,\n",
    "                                                  test_size = 0.2,\n",
    "                                                  random_state = 666)\n",
    "\n",
    "train_encoded_Y = encoder.transform(Y_train)\n",
    "val_encoded_Y = encoder.transform(Y_val)\n",
    "dummy_Y_train = tf.keras.utils.to_categorical(train_encoded_Y) # One Hot Encoded - Convert Integers to Dummy \n",
    "dummy_Y_val = tf.keras.utils.to_categorical(val_encoded_Y)\n",
    "\n",
    "encoded_Y_test = encoder.transform(Y_test)\n",
    "dummy_Y_test = tf.keras.utils.to_categorical(encoded_Y_test)\n",
    "\n",
    "key = pd.DataFrame(index = range(Y.cat.categories.size), \n",
    "                   columns = [\"key_value\", \"sample\"])\n",
    "for i in range(Y.cat.categories.size):\n",
    "    key.loc[i, \"key_value\"] = i\n",
    "    key.loc[i, \"sample\"] = Y.cat.categories[i]\n",
    "key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visible = Input(shape = (5,))\n",
    "\n",
    "transform = RandomFourierFeatures(output_dim = 500,\n",
    "                                  kernel_initializer = \"laplacian\",\n",
    "                                  trainable = True,\n",
    "                                  scale = 0.26) (visible)\n",
    "batch_norm = BatchNormalization() (transform)\n",
    "dropout = Dropout(0.5) (batch_norm)\n",
    "hidden = Dense(250,\n",
    "               activation = \"swish\",\n",
    "               kernel_initializer = \"random_normal\",\n",
    "               kernel_regularizer = l2(l = 0.0001)) (dropout)\n",
    "batch_norm = BatchNormalization() (hidden)\n",
    "dropout = Dropout(0.5) (batch_norm)\n",
    "hidden = Dense(250,\n",
    "               activation = \"swish\",\n",
    "               kernel_initializer = \"random_normal\",\n",
    "               kernel_regularizer = l2(l = 0.0001)) (dropout)\n",
    "batch_norm = BatchNormalization() (hidden)\n",
    "dropout = Dropout(0.2) (batch_norm)\n",
    "hidden = Dense(key.shape[0], activation = \"softmax\") (dropout)\n",
    "model = Model(inputs = visible, outputs = hidden)\n",
    "\n",
    "clr = CyclicLR(base_lr = 0.0001,\n",
    "              max_lr = 0.01,\n",
    "              step_size = 16, # maybe try 8\n",
    "              mode = \"triangular2\")\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adam\", metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(X, dummy_Y,\n",
    "                    epochs = 1000,\n",
    "                    batch_size = 32,\n",
    "                    validation_split = 0.8,\n",
    "                    callbacks = [clr],\n",
    "                    verbose = 0\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract Trained Weights excluding the final layer\n",
    "\n",
    "weights = model.get_weights()\n",
    "NSVM_weights = weights[:19]\n",
    "\n",
    "# create model excluding the final layer\n",
    "\n",
    "visible = Input(shape = (5,))\n",
    "transform = RandomFourierFeatures(output_dim = 500,\n",
    "                                  kernel_initializer = \"laplacian\",\n",
    "                                  trainable = True,\n",
    "                                  scale = 0.26) (visible)\n",
    "batch_norm = BatchNormalization() (transform)\n",
    "dropout = Dropout(0.5) (batch_norm)\n",
    "hidden = Dense(250,\n",
    "               activation = \"swish\",\n",
    "               kernel_initializer = \"random_normal\",\n",
    "               kernel_regularizer = l2(l = 0.0001)) (dropout)\n",
    "batch_norm = BatchNormalization() (hidden)\n",
    "dropout = Dropout(0.5) (batch_norm)\n",
    "hidden = Dense(250,\n",
    "               activation = \"swish\",\n",
    "               kernel_initializer = \"random_normal\",\n",
    "               kernel_regularizer = l2(l = 0.0001)) (dropout)\n",
    "batch_norm = BatchNormalization() (hidden)\n",
    "\n",
    "# redefine model without the final activation\n",
    "\n",
    "neural_model = Model(inputs = visible, outputs = batch_norm)\n",
    "\n",
    "# Restore weights\n",
    "\n",
    "neural_model.set_weights(NSVM_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_model.save(\"Neural_Model.h5\")\n",
    "\n",
    "# to load the weights you have to redefine the model and type neural_model.load_weights(\"Model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trained neural network for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_neural_train = neural_model.predict(X)\n",
    "X_neural_test = neural_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use extracted features to tune a Linear SVM activation layer\n",
    "### Bayesian Hyperparameter optimization of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:52<00:00,  1.12s/trial, best loss: -0.9460000000000001]\n"
     ]
    }
   ],
   "source": [
    "space = {\"C\" : hp.uniform(\"C\", 0, 1000)}\n",
    "def objective(space):\n",
    "    neural_svm = SVC(kernel = \"linear\",\n",
    "                     C = space[\"C\"]\n",
    "                    )\n",
    "    accuracy = cross_val_score(neural_svm, X_neural_train,\n",
    "                               Y_encoded, cv = 10).mean()\n",
    "    return {\"loss\": -accuracy, \"status\": STATUS_OK }\n",
    "trials = Trials()\n",
    "best = fmin(fn = objective,\n",
    "            space = space,\n",
    "            algo = tpe.suggest,\n",
    "            max_evals = 100,\n",
    "            trials = trials\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Training of SVM Activation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95        85\n",
      "           1       0.93      0.94      0.93        84\n",
      "           2       0.98      0.91      0.94        87\n",
      "           3       0.95      0.96      0.95        77\n",
      "\n",
      "    accuracy                           0.95       333\n",
      "   macro avg       0.95      0.95      0.95       333\n",
      "weighted avg       0.95      0.95      0.95       333\n",
      "\n",
      "[[83  1  0  1]\n",
      " [ 2 79  2  1]\n",
      " [ 1  5 79  2]\n",
      " [ 3  0  0 74]]\n"
     ]
    }
   ],
   "source": [
    "NSVM = SVC(kernel = \"linear\",\n",
    "           C = best[\"C\"],\n",
    "           probability = True,\n",
    "          ).fit(X_neural_train, Y_encoded)\n",
    "\n",
    "prediction_NSVM = NSVM.predict(X_neural_test)\n",
    "accuracy = classification_report(encoded_Y_test, prediction_NSVM)\n",
    "\n",
    "print(accuracy)\n",
    "print(confusion_matrix(encoded_Y_test, prediction_NSVM))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
